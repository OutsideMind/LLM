import streamlit as st
import time
import torch
from typing import Dict, List, Optional
from pathlib import Path
from src.config import TRAINING_DATA_PATH, SD_MODEL_NAME, LORA_MODELS
from src.model_trainer import ModelTrainer


def render_training_interface():
    """–û—Å–Ω–æ–≤–Ω–æ–π –∏–Ω—Ç–µ—Ä—Ñ–µ–π—Å –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è –∏ —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è LoRA –º–æ–¥–µ–ª—è–º–∏"""
    st.header("üîÑ Model Training Dashboard")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ GPU
    if not torch.cuda.is_available():
        st.error("‚ö†Ô∏è –¢—Ä–µ–±—É–µ—Ç—Å—è GPU —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π CUDA")
        st.info("–û–±—É—á–µ–Ω–∏–µ –Ω–µ–≤–æ–∑–º–æ–∂–Ω–æ –±–µ–∑ NVIDIA GPU")
        return

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Ç—Ä–µ–Ω–µ—Ä–∞
    if "model_trainer" not in st.session_state:
        if "image_generator" in st.session_state and st.session_state.image_generator.text2img_pipe:
            st.session_state.model_trainer = ModelTrainer(
                st.session_state.image_generator.text2img_pipe
            )
        else:
            st.warning("–°–Ω–∞—á–∞–ª–∞ –∑–∞–≥—Ä—É–∑–∏—Ç–µ –º–æ–¥–µ–ª—å –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π")
            return

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≥–æ—Ç–æ–≤–Ω–æ—Å—Ç–∏ –ø–∞–π–ø–ª–∞–π–Ω–∞
    if not st.session_state.model_trainer.is_ready():
        st.warning("–ú–æ–¥–µ–ª—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –Ω–µ –∑–∞–≥—Ä—É–∂–µ–Ω–∞!")
        if st.button("–ó–∞–≥—Ä—É–∑–∏—Ç—å –º–æ–¥–µ–ª—å –ø–æ —É–º–æ–ª—á–∞–Ω–∏—é"):
            st.session_state.image_generator.load_models()
            st.rerun()
        return

    tab_train, tab_clone, tab_manage = st.tabs([
        "üèãÔ∏è Train New LoRA",
        "üß¨ Clone Model",
        "üóÇÔ∏è Manage Models"
    ])

    with tab_train:
        render_training_tab()

    with tab_clone:
        render_cloning_tab()

    with tab_manage:
        render_management_tab()


def render_training_tab():
    """–í–∫–ª–∞–¥–∫–∞ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –Ω–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    st.subheader("Create New LoRA Model")

    with st.form("training_form", clear_on_submit=False):
        col1, col2 = st.columns(2)

        with col1:
            lora_name = st.text_input(
                "Model Name",
                value="my_style",
                help="–£–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –¥–ª—è –≤–∞—à–µ–π –º–æ–¥–µ–ª–∏"
            )
            placeholder = st.text_input(
                "Trigger Phrase",
                value="<my-style>",
                help="–¢–æ–∫–µ–Ω –¥–ª—è –∞–∫—Ç–∏–≤–∞—Ü–∏–∏ —Å—Ç–∏–ª—è –≤ –ø—Ä–æ–º–ø—Ç–µ"
            )

            # –í—ã–±–æ—Ä —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è
            available_loras = get_available_loras()
            target_lora = st.selectbox(
                "Continue Training (optional)",
                options=[""] + list(available_loras.keys()),
                format_func=lambda x: x if x else "-- Create New --",
                help="–í—ã–±–µ—Ä–∏—Ç–µ —Å—É—â–µ—Å—Ç–≤—É—é—â—É—é LoRA –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è"
            )

        with col2:
            # –†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –æ–±—É—á–µ–Ω–∏—è
            with st.expander("‚öôÔ∏è Advanced Settings", expanded=True):
                epochs = st.slider("Epochs", 10, 200, 50)
                lr = st.slider(
                    "Learning Rate",
                    1e-6, 1e-3, 1e-4,
                    format="%.0e"
                )
                lora_rank = st.slider("LoRA Rank", 4, 64, 8)
                lora_alpha = st.slider("LoRA Alpha", 8, 128, 32)
                batch_size = st.selectbox("Batch Size", [1, 2, 4], index=0)
                resolution = st.select_slider(
                    "Resolution",
                    options=[512, 640, 768, 896, 1024],
                    value=768
                )

            uploaded_images = st.file_uploader(
                "Training Images (5-20 images)",
                type=["jpg", "png", "jpeg"],
                accept_multiple_files=True,
                help="–ò–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –æ–¥–Ω–æ–º —Å—Ç–∏–ª–µ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è"
            )

        # –ö–Ω–æ–ø–∫–∞ –∑–∞–ø—É—Å–∫–∞ –æ–±—É—á–µ–Ω–∏—è
        submit = st.form_submit_button("Start Training")

        if submit:
            validate_and_start_training(
                lora_name, placeholder, uploaded_images,
                epochs, lr, lora_rank, lora_alpha,
                batch_size, resolution, target_lora
            )

    # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è
    render_training_progress()


def render_cloning_tab():
    """–í–∫–ª–∞–¥–∫–∞ –¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–µ–π"""
    st.subheader("Clone Existing LoRA Model")
    st.info("–°–æ–∑–¥–∞–π—Ç–µ –∫–æ–ø–∏—é —Å—É—â–µ—Å—Ç–≤—É—é—â–µ–π –º–æ–¥–µ–ª–∏ –¥–ª—è —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–æ–≤")

    available_loras = get_available_loras()

    if not available_loras:
        st.warning("–ù–µ—Ç –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∏—è")
        return

    with st.form("clone_form"):
        source_lora = st.selectbox(
            "Source Model",
            options=list(available_loras.keys()),
            help="–ú–æ–¥–µ–ª—å –∫–æ—Ç–æ—Ä—É—é –±—É–¥–µ–º –∫–ª–æ–Ω–∏—Ä–æ–≤–∞—Ç—å"
        )

        new_name = st.text_input(
            "New Model Name",
            value=f"{source_lora}_copy",
            help="–£–Ω–∏–∫–∞–ª—å–Ω–æ–µ –∏–º—è –¥–ª—è –Ω–æ–≤–æ–π –º–æ–¥–µ–ª–∏"
        )

        new_placeholder = st.text_input(
            "New Trigger Phrase",
            value=f"<{source_lora}-copy>",
            help="–ù–æ–≤—ã–π —Ç–æ–∫–µ–Ω –∞–∫—Ç–∏–≤–∞—Ü–∏–∏"
        )

        if st.form_submit_button("Clone Model"):
            if new_name in available_loras:
                st.error(f"–ú–æ–¥–µ–ª—å '{new_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç!")
            else:
                with st.spinner("Cloning model..."):
                    success = st.session_state.image_generator.clone_lora(
                        source_name=source_lora,
                        new_name=new_name,
                        new_placeholder=new_placeholder
                    )

                    if success:
                        st.success(f"–ú–æ–¥–µ–ª—å {source_lora} —É—Å–ø–µ—à–Ω–æ –∫–ª–æ–Ω–∏—Ä–æ–≤–∞–Ω–∞ –∫–∞–∫ {new_name}")
                        st.balloons()
                        time.sleep(1)
                        st.rerun()


def render_management_tab():
    """–í–∫–ª–∞–¥–∫–∞ –¥–ª—è —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏"""
    st.subheader("LoRA Model Management")
    available_loras = get_available_loras()

    if not available_loras:
        st.info("–ù–µ—Ç —Å–æ–∑–¥–∞–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π")
        return

    # –¢–∞–±–ª–∏—Ü–∞ —Å –º–æ–¥–µ–ª—è–º–∏
    st.write("### Available Models")
    for model_name, config in available_loras.items():
        with st.expander(f"üìÅ {model_name}"):
            col1, col2 = st.columns([4, 1])
            with col1:
                st.json({
                    "Placeholder": config.get("placeholder", ""),
                    "Description": config.get("description", ""),
                    "Created": config.get("created", ""),
                    "Filename": config.get("filename", ""),
                    "Base Model": config.get("base_model", SD_MODEL_NAME)
                }, expanded=False)

            with col2:
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º, —á—Ç–æ –º–æ–¥–µ–ª—å –Ω–µ —è–≤–ª—è–µ—Ç—Å—è –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω–æ–π
                if model_name not in LORA_MODELS:
                    if st.button("üóëÔ∏è", key=f"del_{model_name}", help="–£–¥–∞–ª–∏—Ç—å –º–æ–¥–µ–ª—å"):
                        delete_lora_model(model_name)


def render_training_progress():
    """–û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    if hasattr(st.session_state, "training_started") and st.session_state.training_started:
        trainer = st.session_state.model_trainer
        progress = trainer.get_training_progress()

        st.subheader("Training Progress")

        # –ü—Ä–æ–≥—Ä–µ—Å—Å –±–∞—Ä –∏ –º–µ—Ç—Ä–∏–∫–∏
        progress_col, metrics_col = st.columns([3, 1])

        with progress_col:
            st.progress(progress["progress"])

            if progress["loss_history"]:
                st.line_chart({"Loss": progress["loss_history"]})

        with metrics_col:
            st.metric("Epoch", f"{progress['epoch']}")
            st.metric("Current LoRA", progress.get("current_lora", ""))

        # –ö–Ω–æ–ø–∫–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏
        if st.button("‚õî Stop Training"):
            trainer.stop_training()
            st.session_state.training_started = False
            st.rerun()


def get_available_loras() -> Dict:
    """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö LoRA –º–æ–¥–µ–ª–µ–π"""
    if "image_generator" in st.session_state:
        return st.session_state.image_generator.all_loras
    return {}


def delete_lora_model(model_name: str):
    """–£–¥–∞–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ–º"""
    # –ó–∞—â–∏—Ç–∞ –æ—Ç —É–¥–∞–ª–µ–Ω–∏—è –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π
    if model_name in LORA_MODELS:
        st.error("–ù–µ–ª—å–∑—è —É–¥–∞–ª—è—Ç—å –ø—Ä–µ–¥—É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏!")
        return

    if st.session_state.image_generator.delete_dynamic_lora(model_name):
        st.success(f"–ú–æ–¥–µ–ª—å {model_name} —É–¥–∞–ª–µ–Ω–∞!")
        time.sleep(1)
        st.rerun()
    else:
        st.error("–û—à–∏–±–∫–∞ –ø—Ä–∏ —É–¥–∞–ª–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏")


def validate_and_start_training(
        lora_name: str,
        placeholder: str,
        uploaded_images: List,
        epochs: int,
        lr: float,
        lora_rank: int,
        lora_alpha: int,
        batch_size: int,
        resolution: int,
        target_lora: Optional[str] = None
):
    """–í–∞–ª–∏–¥–∞—Ü–∏—è –∏ –∑–∞–ø—É—Å–∫ –ø—Ä–æ—Ü–µ—Å—Å–∞ –æ–±—É—á–µ–Ω–∏—è"""
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    errors = []

    if not lora_name:
        errors.append("–£–∫–∞–∂–∏—Ç–µ –∏–º—è –º–æ–¥–µ–ª–∏")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ placeholder
    if not placeholder.strip() or not placeholder.startswith("<") or not placeholder.endswith(">"):
        st.warning("‚ö†Ô∏è Trigger phrase should be in format <your-style>")
        placeholder = f"<{lora_name}-style>"  # –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ
        st.info(f"Using auto-generated trigger: {placeholder}")

    if len(uploaded_images) < 3:
        errors.append("–ó–∞–≥—Ä—É–∑–∏—Ç–µ –º–∏–Ω–∏–º—É–º 3 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è")
    elif len(uploaded_images) > 30:
        errors.append("–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π (–º–∞–∫—Å. 30)")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—É—â–µ—Å—Ç–≤–æ–≤–∞–Ω–∏—è –∏–º–µ–Ω–∏
    if lora_name in get_available_loras():
        errors.append(f"–ú–æ–¥–µ–ª—å '{lora_name}' —É–∂–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç")

    # –í—ã–≤–æ–¥ –æ—à–∏–±–æ–∫
    if errors:
        for error in errors:
            st.error(error)
        return

    # –°–æ–∑–¥–∞–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω–æ–π –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
    session_id = str(int(time.time()))
    temp_folder = Path(TRAINING_DATA_PATH) / session_id
    temp_folder.mkdir(parents=True, exist_ok=True)

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π
    for i, img in enumerate(uploaded_images):
        img_path = temp_folder / f"img_{i}.jpg"
        with open(img_path, "wb") as f:
            f.write(img.getbuffer())

    # –ó–∞–ø—É—Å–∫ –æ–±—É—á–µ–Ω–∏—è
    with st.spinner("Starting training process..."):
        st.session_state.training_started = True

        success = st.session_state.model_trainer.train_model(
            dataset_path=str(temp_folder),
            placeholder_token=placeholder,
            lora_name=lora_name,
            epochs=epochs,
            lr=lr,
            batch_size=batch_size,
            resolution=resolution,
            lora_rank=lora_rank,
            lora_alpha=lora_alpha,
            target_lora=target_lora if target_lora else None
        )

        if success:
            st.success("–û–±—É—á–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–æ!")
            st.balloons()
        else:
            st.error("–û—à–∏–±–∫–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ –æ–±—É—á–µ–Ω–∏—è")